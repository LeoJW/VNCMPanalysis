{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2a6aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.feather as feather\n",
    "from pycircstat2 import Circular\n",
    "\n",
    "def load_dicts_from_h5(filename):\n",
    "    d = {}\n",
    "    with h5py.File(filename, 'r') as f:\n",
    "        for key in f.keys():\n",
    "            dataset = f[key]\n",
    "            if dataset.dtype.char == 'S':  # String data\n",
    "                d[key] = dataset[()].decode('utf-8') if isinstance(dataset[()], bytes) else dataset[()]\n",
    "            else:  # Numeric data\n",
    "                d[key] = dataset[()]\n",
    "    return d\n",
    "\n",
    "def save_dicts_to_h5(dicts, filename):\n",
    "    with h5py.File(filename, 'w') as f:\n",
    "        for i, d in enumerate(dicts):\n",
    "            group = f.create_group(f'dict_{i}')\n",
    "            \n",
    "            for key, value in d.items():\n",
    "                if isinstance(value, np.ndarray):\n",
    "                    group.create_dataset(key, data=value)\n",
    "                else:\n",
    "                    # Store strings as fixed-length or variable-length\n",
    "                    group.create_dataset(key, data=value)\n",
    "\n",
    "main_dir = os.getcwd()\n",
    "data_dir = os.path.join(main_dir, '..', 'localdata')\n",
    "\n",
    "moths = [\n",
    "    \"2025-02-25_1\",\n",
    "    \"2025-02-25\",\n",
    "    \"2025-03-11\",\n",
    "    \"2025-03-12_1\",\n",
    "    \"2025-03-20\",\n",
    "    \"2025-03-21\"\n",
    "]\n",
    "fsamp = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0c74c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size is large (n>10000), it will take a while to find the median.\n",
      "Or set `kwargs_median={'method': None}` to skip.\n",
      "Sample size is large (n>10000), it will take a while to find the median.\n",
      "Or set `kwargs_median={'method': None}` to skip.\n",
      "Sample size is large (n>10000), it will take a while to find the median.\n",
      "Or set `kwargs_median={'method': None}` to skip.\n",
      "Sample size is large (n>10000), it will take a while to find the median.\n",
      "Or set `kwargs_median={'method': None}` to skip.\n",
      "Sample size is large (n>10000), it will take a while to find the median.\n",
      "Or set `kwargs_median={'method': None}` to skip.\n",
      "Sample size is large (n>10000), it will take a while to find the median.\n",
      "Or set `kwargs_median={'method': None}` to skip.\n",
      "Sample size is large (n>10000), it will take a while to find the median.\n",
      "Or set `kwargs_median={'method': None}` to skip.\n",
      "Sample size is large (n>10000), it will take a while to find the median.\n",
      "Or set `kwargs_median={'method': None}` to skip.\n",
      "Sample size is large (n>10000), it will take a while to find the median.\n",
      "Or set `kwargs_median={'method': None}` to skip.\n",
      "Sample size is large (n>10000), it will take a while to find the median.\n",
      "Or set `kwargs_median={'method': None}` to skip.\n",
      "Sample size is large (n>10000), it will take a while to find the median.\n",
      "Or set `kwargs_median={'method': None}` to skip.\n",
      "Sample size is large (n>10000), it will take a while to find the median.\n",
      "Or set `kwargs_median={'method': None}` to skip.\n",
      "Sample size is large (n>10000), it will take a while to find the median.\n",
      "Or set `kwargs_median={'method': None}` to skip.\n",
      "Sample size is large (n>10000), it will take a while to find the median.\n",
      "Or set `kwargs_median={'method': None}` to skip.\n",
      "Sample size is large (n>10000), it will take a while to find the median.\n",
      "Or set `kwargs_median={'method': None}` to skip.\n",
      "Sample size is large (n>10000), it will take a while to find the median.\n",
      "Or set `kwargs_median={'method': None}` to skip.\n",
      "Sample size is large (n>10000), it will take a while to find the median.\n",
      "Or set `kwargs_median={'method': None}` to skip.\n",
      "Sample size is large (n>10000), it will take a while to find the median.\n",
      "Or set `kwargs_median={'method': None}` to skip.\n",
      "Sample size is large (n>10000), it will take a while to find the median.\n",
      "Or set `kwargs_median={'method': None}` to skip.\n",
      "Sample size is large (n>10000), it will take a while to find the median.\n",
      "Or set `kwargs_median={'method': None}` to skip.\n",
      "Sample size is large (n>10000), it will take a while to find the median.\n",
      "Or set `kwargs_median={'method': None}` to skip.\n",
      "Sample size is large (n>10000), it will take a while to find the median.\n",
      "Or set `kwargs_median={'method': None}` to skip.\n",
      "Sample size is large (n>10000), it will take a while to find the median.\n",
      "Or set `kwargs_median={'method': None}` to skip.\n",
      "Sample size is large (n>10000), it will take a while to find the median.\n",
      "Or set `kwargs_median={'method': None}` to skip.\n",
      "Sample size is large (n>10000), it will take a while to find the median.\n",
      "Or set `kwargs_median={'method': None}` to skip.\n",
      "Sample size is large (n>10000), it will take a while to find the median.\n",
      "Or set `kwargs_median={'method': None}` to skip.\n",
      "Sample size is large (n>10000), it will take a while to find the median.\n",
      "Or set `kwargs_median={'method': None}` to skip.\n",
      "Sample size is large (n>10000), it will take a while to find the median.\n",
      "Or set `kwargs_median={'method': None}` to skip.\n"
     ]
    }
   ],
   "source": [
    "ref_muscle = 'ldlm'\n",
    "wb_duration_thresholds = np.array([50, 70]) # ms ldlm\n",
    "\n",
    "rows = []\n",
    "# Loop over moths\n",
    "for moth in moths:\n",
    "    # Load each moth's data\n",
    "    base_name = os.path.join(data_dir, moth)\n",
    "    data_file = f\"{base_name}_data.npz\"\n",
    "    labels_file = f\"{base_name}_labels.npz\"\n",
    "    spikes = np.load(os.path.join(data_file))\n",
    "    labels = {}\n",
    "    if os.path.exists(labels_file):\n",
    "        labels_data = np.load(labels_file)\n",
    "        labels = {unit: labels_data[unit].item() for unit in labels_data.files}\n",
    "    units = spikes.files  # List of unit labels\n",
    "    neurons, neuron_quality, muscle_labels = [], [], []\n",
    "    for unit in units:\n",
    "        if unit.isnumeric():  # Numeric labels are neurons\n",
    "            label = labels.get(unit, None)\n",
    "            neurons.append(unit)\n",
    "            neuron_quality.append(label)\n",
    "        else:  # Alphabetic labels are muscles\n",
    "            muscle_labels.append(unit)\n",
    "    neurons = np.array(neurons)\n",
    "    neuron_quality = np.array(neuron_quality)\n",
    "\n",
    "    # Extract DLM phase, put all neuron spikes on that phase\n",
    "    diffvec = np.diff(spikes[ref_muscle]) / fsamp * 1000 # units of ms\n",
    "    mask = np.logical_and(diffvec > wb_duration_thresholds[0], diffvec < wb_duration_thresholds[1])\n",
    "    start_inds = spikes[ref_muscle][np.where(np.hstack([False, mask]))[0]]\n",
    "    wblen = np.hstack([np.diff(start_inds), 10000])\n",
    "\n",
    "    # For each neuron, assign spikes to wingstrokes\n",
    "    max_duration_thresh_samples = wb_duration_thresholds[1] / 1000 * fsamp\n",
    "    phase_dict, wblen_dict = {}, {}\n",
    "    for neur in neurons:\n",
    "        wb_assign = np.searchsorted(start_inds, spikes[neur], side='right') - 1\n",
    "        # Spikes which are in valid wingbeats (not before first wb or after last, not too long of a wingbeat)\n",
    "        mask = np.logical_and(wb_assign >= 0, wb_assign <= len(start_inds))\n",
    "        # New spike vector that's spike index - wingbeat start index / wingbeat length\n",
    "        phase = (spikes[neur][mask] - start_inds[wb_assign[mask]]) / wblen[wb_assign[mask]]\n",
    "        length_mask = wblen[wb_assign[mask]] < max_duration_thresh_samples\n",
    "        phase = phase[length_mask]\n",
    "        phase_dict[neur] = phase\n",
    "        wblen_dict[neur] = wblen[wb_assign[mask]][length_mask] / fsamp * 1000\n",
    "\n",
    "    # Loop over neurons\n",
    "    for neur in neurons:\n",
    "        # Too few spikes for good stats\n",
    "        if len(phase_dict[neur]) < 100:\n",
    "            rows.append({\n",
    "                \"moth\": moth, \n",
    "                \"neuron\": int(neur),\n",
    "                \"r_sharpness\": 0,\n",
    "                \"kappa_global\": 0,\n",
    "                \"r_rayleigh\": 0,\n",
    "                \"s_deviation\": 0,\n",
    "                \"mean\": 0,\n",
    "                \"mean_pvalue\": 0,\n",
    "                \"median\": 0,\n",
    "                \"n_clusters\": 0,\n",
    "                \"mu\": [0],\n",
    "                \"kappa\": [0],\n",
    "            })\n",
    "        else:\n",
    "            # Use mixture of von mises distributions and BIC to determine how many preferred directions each neuron has\n",
    "            c = Circular(phase_dict[neur] * 2 * np.pi, unit=\"radian\", n_clusters_max=7)\n",
    "            # For each, save the following:\n",
    "            rows.append({\n",
    "                \"moth\": moth, \n",
    "                \"neuron\": int(neur),\n",
    "                \"r_sharpness\": c.r, # r (resultant vector length, sharpness)\n",
    "                \"kappa_global\": c.kappa, # kappa (concentration param) for all data\n",
    "                \"r_rayleigh\": c.R, # R (Rayleigh's R stat)\n",
    "                \"s_deviation\": c.s, # s (angular deviation, measure of dispersion)\n",
    "                \"mean\": c.mean, # mean angle\n",
    "                \"mean_pvalue\": c.mean_test_result.pval, # p value of mean angle from Rayleigh test\n",
    "                \"median\": c.median,\n",
    "                \"n_clusters\": c.mixture_opt.n_clusters, # Optimal number of clusters from mixture of Von Mises fitting + BIC\n",
    "                \"mu\": [a['mu'] for a in c.mixture_opt.params_], # mean parameter for each von mises distribution\n",
    "                \"kappa\": [a['kappa'] for a in c.mixture_opt.params_], # kappa parameter for each von mises distribution\n",
    "            })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "table = pa.Table.from_pandas(df)\n",
    "feather.write_feather(table, os.path.join(data_dir, \"circular_stats.arrow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a20298f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_muscle = 'ldlm'\n",
    "wb_duration_thresholds = np.array([50, 70]) # ms ldlm\n",
    "\n",
    "rows = []\n",
    "# Loop over moths\n",
    "for moth in moths:\n",
    "    # Load each moth's data\n",
    "    base_name = os.path.join(data_dir, moth)\n",
    "    data_file = f\"{base_name}_data.npz\"\n",
    "    labels_file = f\"{base_name}_labels.npz\"\n",
    "    spikes = np.load(os.path.join(data_file))\n",
    "    labels = {}\n",
    "    if os.path.exists(labels_file):\n",
    "        labels_data = np.load(labels_file)\n",
    "        labels = {unit: labels_data[unit].item() for unit in labels_data.files}\n",
    "    units = spikes.files  # List of unit labels\n",
    "    neurons, neuron_quality, muscle_labels = [], [], []\n",
    "    for unit in units:\n",
    "        if unit.isnumeric():  # Numeric labels are neurons\n",
    "            label = labels.get(unit, None)\n",
    "            neurons.append(unit)\n",
    "            neuron_quality.append(label)\n",
    "        else:  # Alphabetic labels are muscles\n",
    "            muscle_labels.append(unit)\n",
    "    neurons = np.array(neurons)\n",
    "    neuron_quality = np.array(neuron_quality)\n",
    "\n",
    "    # Extract DLM phase, put all neuron spikes on that phase\n",
    "    diffvec = np.diff(spikes[ref_muscle]) / fsamp * 1000 # units of ms\n",
    "    mask = np.logical_and(diffvec > wb_duration_thresholds[0], diffvec < wb_duration_thresholds[1])\n",
    "    start_inds = spikes[ref_muscle][np.where(np.hstack([False, mask]))[0]]\n",
    "    wblen = np.hstack([np.diff(start_inds), 10000])\n",
    "\n",
    "    # For each neuron, assign spikes to wingstrokes\n",
    "    max_duration_thresh_samples = wb_duration_thresholds[1] / 1000 * fsamp\n",
    "    phase_dict, wblen_dict = {}, {}\n",
    "    for neur in neurons:\n",
    "        wb_assign = np.searchsorted(start_inds, spikes[neur], side='right') - 1\n",
    "        # Spikes which are in valid wingbeats (not before first wb or after last, not too long of a wingbeat)\n",
    "        mask = np.logical_and(wb_assign >= 0, wb_assign <= len(start_inds))\n",
    "        # New spike vector that's spike index - wingbeat start index / wingbeat length\n",
    "        phase = (spikes[neur][mask] - start_inds[wb_assign[mask]]) / wblen[wb_assign[mask]]\n",
    "        length_mask = wblen[wb_assign[mask]] < max_duration_thresh_samples\n",
    "        phase = phase[length_mask]\n",
    "        phase_dict[neur] = phase\n",
    "        wblen_dict[neur] = wblen[wb_assign[mask]][length_mask] / fsamp * 1000\n",
    "\n",
    "    # Loop over neurons\n",
    "    for neur in neurons:\n",
    "        # Too few spikes for good stats\n",
    "        if len(phase_dict[neur]) < 100:\n",
    "            rows.append({\n",
    "                \"moth\": moth, \n",
    "                \"neuron\": int(neur),\n",
    "                \"r_sharpness\": 0,\n",
    "                \"kappa_global\": 0,\n",
    "                \"r_rayleigh\": 0,\n",
    "                \"s_deviation\": 0,\n",
    "                \"mean\": 0,\n",
    "                \"mean_pvalue\": 0,\n",
    "                \"median\": 0,\n",
    "                \"n_clusters\": 0,\n",
    "                \"mu\": [0],\n",
    "                \"kappa\": [0],\n",
    "            })\n",
    "        else:\n",
    "            # Use mixture of von mises distributions and BIC to determine how many preferred directions each neuron has\n",
    "            c = Circular(phase_dict[neur] * 2 * np.pi, unit=\"radian\", n_clusters_max=7)\n",
    "            # For each, save the following:\n",
    "            rows.append({\n",
    "                \"moth\": moth, \n",
    "                \"neuron\": int(neur),\n",
    "                \"r_sharpness\": c.r, # r (resultant vector length, sharpness)\n",
    "                \"kappa_global\": c.kappa, # kappa (concentration param) for all data\n",
    "                \"r_rayleigh\": c.R, # R (Rayleigh's R stat)\n",
    "                \"s_deviation\": c.s, # s (angular deviation, measure of dispersion)\n",
    "                \"mean\": c.mean, # mean angle\n",
    "                \"mean_pvalue\": c.mean_test_result.pval, # p value of mean angle from Rayleigh test\n",
    "                \"median\": c.median,\n",
    "                \"n_clusters\": c.mixture_opt.n_clusters, # Optimal number of clusters from mixture of Von Mises fitting + BIC\n",
    "                \"mu\": [a['mu'] for a in c.mixture_opt.params_], # mean parameter for each von mises distribution\n",
    "                \"kappa\": [a['kappa'] for a in c.mixture_opt.params_], # kappa parameter for each von mises distribution\n",
    "            })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "table = pa.Table.from_pandas(df)\n",
    "feather.write_feather(table, os.path.join(data_dir, \"circular_stats.arrow\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
